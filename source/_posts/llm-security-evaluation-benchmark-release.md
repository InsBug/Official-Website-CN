---
title: 大语言模型（LLM）安全性测评基准发布
slug: llm-security-evaluation-benchmark-release
author: 洞源实验室
date: 2023-11-24 18:00
categories: AI安全
tags:
  - 大语言模型
  - LLM
  - 安全测评
  - 测试基准
  - AI安全
  - 模型评估
---

2023年8月15日国家六个部委发布的《生成式人工智能服务管理暂行办法》正式施行，该办法强调了大语言模型安全的重要性，防止生成潜在隐私泄露、违法犯罪内容。

为评估大语言模型应用及服务的安全性，腾讯混元大模型、腾讯朱雀实验室、供应链安全检测中心洞源实验室联合清华大学、OWASP中国推出大语言模型（LLM）安全性测评基准，旨在评估大语言模型在Prompt安全和内容安全方面的能力，为企业的大语言模型应用或服务改进提供建议和指导。

## 编写团队

裴歌（腾讯混元大模型项目组）、冉娜（腾讯朱雀实验室）、裴伟伟（洞源实验室）、马云卓（洞源实验室）

## 指导专家

杨勇（腾讯科技）、江勇（清华大学深圳国际研究生院）、夏树涛（清华大学深圳国际研究生院）、沈蔚锋（长安通信众测平台）、彭泉（广东安证计算机司法鉴定所）

## 基准背景

大语言模型（LLM，Large Language Model）是指参数量巨大、能够处理海量数据的模型, 此类模型通常具有大规模的参数，使得它们能够处理更复杂的问题，并学习更广泛的知识。 

目前市场上出现了越来越多的商用和开源大模型产品和服务，用户使用大模型产品或服务时需要提供 prompt（提示）作为输入，模型会尝试将 prompt 与它所学习到的知识相匹配，从而生成与提示相关的输出。Prompt Engineering（提示工程学）即是通过设计和优化输入提示来提高大型语言模型的性能和效果，它通过提供清晰、简洁和具有针对性的提示，帮助模型更好地理解问题、提供准确的答案，并提高模型的可解释性和可控性。

由于庞大的规模和复杂的结构，**大语言模型也存在多种安全风险**，如 prompt 误导、数据隐私泄露、模型解释性不足等。如果 prompt 与模型所学习到的模式不匹配或存在偏差，模型的输出可能会产生意想不到的结果，这些结果不仅会影响模型的效果，在严肃的商用场景下，还可能对用户和企业带来诸如经济损失、声誉影响等风险。因此，企业和政府对大模型相关产品安全性的关注和重视程度也在逐渐增加。

目前对大模型的全面测评大多关注于大模型基础能力，包括分类、信息抽取、阅读理解、表格问答、逻辑推理、知识问答等方面，而**缺乏全面的安全性测评基准**，这使得公众和企业在使用大语言模型相关产品和服务时缺乏客观的对比和认识。

本测评基准设立的目的是创建一个**全面、客观、中**立的大语言模型安全测评基准，供企业、机构或团队在选择大语言模型产品和服务时作为参考依据。

## 基准范围

本测评基准的范围**仅限于大语言模型**（包括商用服务和开源模型）在用户输入prompt的操作后大语言模型输出相应结果的场景，与OWASP大语言模型应用程序十大风险相比更侧重模型自身的安全性和基于国内法律法规的合规性。

本基准范围**不包括**模型在分类能力、信息抽取能力、阅读理解能力、表格问答能力、逻辑推理能力、知识问答能力等基础能力的表现。

## 基准概要

该基准是大语言模型产品或服务在面对用户进行prompt输入时，可能遇到的恶意prompt以及模型输出内容涉及的风险场景。大语言模型产品或服务在面对各类攻击手法时的鲁棒性，以及出现涉及伦理、道德等输出的内容，决定了在选择使用大模型产品或服务时的安全性和可控性。

基于大语言模型涉及到的安全风险类型以及相应的触发方式，该基准分为以下两个部分：

**prompt安全**

*   指令劫持
    
*   角色扮演
    
*   反向诱导
    
*   进入开发者模式
    
*   DAN（Do Anything Now）
    
*   对抗后缀攻击
    
*   随机噪声攻击
    
*   弱语义攻击
    
**内容安全**

*   网络安全
    
*   训练数据泄露
    
*   个人隐私泄露
    
*   伦理道德
    
*   违法犯罪
    
*   色情暴力
    
## 附录

《大语言模型（LLM）安全测评基准 v1.0》下载：[点击下载](./assets/大语言模型（LLM）安全测评基准V1.0%20发布版.pdf)